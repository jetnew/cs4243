{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9ce36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from utils import *\n",
    "from model_training import DoodleDataset, RealDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06d14ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlock2(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(dim, dim, (7, 7), padding=3, groups=dim)\n",
    "        self.lin1 = nn.Linear(dim, 4 * dim)\n",
    "        self.lin2 = nn.Linear(4 * dim, dim)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.gelu = nn.GELU()\n",
    "    def forward(self, x):\n",
    "        res_inp = x\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # NCHW -> NHWC\n",
    "        x = self.ln(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = x.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "        out = x + res_inp\n",
    "        return out\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes=9, dropout=0.2, block_dims=[192, 384, 768]):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, block_dims[0], kernel_size=2, stride=2),\n",
    "            ConvNeXtBlock2(block_dims[0]),\n",
    "            nn.Conv2d(block_dims[0], block_dims[1], kernel_size=2, stride=2),\n",
    "            ConvNeXtBlock2(block_dims[1]),\n",
    "            nn.Conv2d(block_dims[1], block_dims[2], kernel_size=2, stride=2),\n",
    "            ConvNeXtBlock2(block_dims[2]),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.block_dims = block_dims\n",
    "        self.project = nn.Linear(block_dims[-1], n_classes)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        x = self.blocks(x)\n",
    "        feats = self.flatten(x)\n",
    "        print (\"feats size: \", feats.shape)\n",
    "        out = self.project(feats)\n",
    "        if return_feats:\n",
    "            return out, feats\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4faf12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats size:  torch.Size([100, 768])\n",
      "torch.Size([100, 9]) torch.Size([100, 768])\n"
     ]
    }
   ],
   "source": [
    "net = ConvNeXt(3, 9)\n",
    "x = torch.rand(100, 3, 64, 64)\n",
    "y, feats = net(x, return_feats=True)\n",
    "print (y.shape, feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44deece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ConvNeXt                                 --                        --\n",
      "├─Sequential: 1-1                        [512, 768, 8, 8]          --\n",
      "│    └─Conv2d: 2-1                       [512, 192, 32, 32]        2,496\n",
      "│    └─ConvNeXtBlock: 2-2                [512, 192, 32, 32]        --\n",
      "│    │    └─Conv2d: 3-1                  [512, 192, 32, 32]        9,600\n",
      "│    │    └─LayerNorm: 3-2               [512, 32, 32, 192]        384\n",
      "│    │    └─Linear: 3-3                  [512, 32, 32, 768]        148,224\n",
      "│    │    └─Linear: 3-4                  [512, 32, 32, 192]        147,648\n",
      "│    │    └─GELU: 3-5                    [512, 32, 32, 192]        --\n",
      "│    └─Conv2d: 2-3                       [512, 384, 16, 16]        295,296\n",
      "│    └─ConvNeXtBlock: 2-4                [512, 384, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-6                  [512, 384, 16, 16]        19,200\n",
      "│    │    └─LayerNorm: 3-7               [512, 16, 16, 384]        768\n",
      "│    │    └─Linear: 3-8                  [512, 16, 16, 1536]       591,360\n",
      "│    │    └─Linear: 3-9                  [512, 16, 16, 384]        590,208\n",
      "│    │    └─GELU: 3-10                   [512, 16, 16, 384]        --\n",
      "│    └─Conv2d: 2-5                       [512, 768, 8, 8]          1,180,416\n",
      "│    └─ConvNeXtBlock: 2-6                [512, 768, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-11                 [512, 768, 8, 8]          38,400\n",
      "│    │    └─LayerNorm: 3-12              [512, 8, 8, 768]          1,536\n",
      "│    │    └─Linear: 3-13                 [512, 8, 8, 3072]         2,362,368\n",
      "│    │    └─Linear: 3-14                 [512, 8, 8, 768]          2,360,064\n",
      "│    │    └─GELU: 3-15                   [512, 8, 8, 768]          --\n",
      "├─Linear: 1-2                            [512, 9]                  6,921\n",
      "==========================================================================================\n",
      "Total params: 7,754,889\n",
      "Trainable params: 7,754,889\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 90.68\n",
      "==========================================================================================\n",
      "Input size (MB): 25.17\n",
      "Forward/backward pass size (MB): 11274.33\n",
      "Params size (MB): 31.02\n",
      "Estimated Total Size (MB): 11330.51\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print (summary(net, input_size=(512, 3, 64, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acfe35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset at 'dataset/cifar/cifar.npy'.\n",
      "Loaded dataset at 'dataset/sketchy/sketchy_real.npy'.\n",
      "Loaded dataset at 'dataset/google_images/google_real.npy'.\n",
      "Loaded dataset at 'dataset/cifar/cifar.npy'.\n",
      "Loaded dataset at 'dataset/sketchy/sketchy_real.npy'.\n",
      "Loaded dataset at 'dataset/google_images/google_real.npy'.\n"
     ]
    }
   ],
   "source": [
    "real_train_set = RealDataset(train=True)\n",
    "real_val_set = RealDataset(train=False)\n",
    "\n",
    "real_train_loader = torch.utils.data.DataLoader(real_train_set, batch_size=512, shuffle=True)\n",
    "real_val_loader = torch.utils.data.DataLoader(real_val_set, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ccb885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11141, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print (real_train_set.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87131fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = ConvNeXt(3, 9)\n",
    "optim = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8bfb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(0)  # zero seed by default\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d98a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, label):\n",
    "    pred, label = pred.cpu(), label.cpu()\n",
    "    return (pred.argmax(1) == label).sum().item() / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0c492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.1568497852845625, Avg. Validation Accuracy: 0.3599145171184856\n",
      "Epoch: 1, Training Loss: 1.6310477690263228, Avg. Validation Accuracy: 0.43760475395828463\n",
      "Epoch: 2, Training Loss: 1.484411207112399, Avg. Validation Accuracy: 0.5142387787742463\n",
      "Epoch: 3, Training Loss: 1.3478566733273594, Avg. Validation Accuracy: 0.5518443086877775\n",
      "Epoch: 4, Training Loss: 1.2495673190463672, Avg. Validation Accuracy: 0.5675514686550596\n",
      "Epoch: 5, Training Loss: 1.1609799699349836, Avg. Validation Accuracy: 0.5801586874123627\n",
      "Epoch: 6, Training Loss: 1.0637939978729596, Avg. Validation Accuracy: 0.6205451039962608\n",
      "Epoch: 7, Training Loss: 1.0163520926778966, Avg. Validation Accuracy: 0.6147414151963075\n",
      "Epoch: 8, Training Loss: 0.937765511599454, Avg. Validation Accuracy: 0.6923878333868895\n",
      "Epoch: 9, Training Loss: 0.8219836245883595, Avg. Validation Accuracy: 0.752622728733349\n",
      "Epoch: 10, Training Loss: 0.7162175693295219, Avg. Validation Accuracy: 0.7968396255696425\n",
      "Epoch: 11, Training Loss: 0.6138458116488024, Avg. Validation Accuracy: 0.8317046641300538\n",
      "Epoch: 12, Training Loss: 0.5169127190654929, Avg. Validation Accuracy: 0.8660920337695722\n",
      "Epoch: 13, Training Loss: 0.41642980683933606, Avg. Validation Accuracy: 0.9048085490184622\n",
      "Epoch: 14, Training Loss: 0.3365632566538724, Avg. Validation Accuracy: 0.9027967709307081\n",
      "Epoch: 15, Training Loss: 0.27188144759698346, Avg. Validation Accuracy: 0.9454567363870062\n",
      "Epoch: 16, Training Loss: 0.19898891923102466, Avg. Validation Accuracy: 0.9649501015132039\n",
      "Epoch: 17, Training Loss: 0.11729585243897005, Avg. Validation Accuracy: 0.9837953157863987\n",
      "Epoch: 18, Training Loss: 0.07631567526947368, Avg. Validation Accuracy: 0.9824917109721898\n",
      "Epoch: 19, Training Loss: 0.055467178198424255, Avg. Validation Accuracy: 0.9952059659090909\n"
     ]
    }
   ],
   "source": [
    "epochs= 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    count = 0\n",
    "    for idx, (x, y) in enumerate(real_train_loader):\n",
    "        count += 1\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        total_loss += loss.detach().cpu().item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    epoch_loss = total_loss / count\n",
    "    \n",
    "    total_val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for idx, (x, y) in enumerate(real_val_loader):\n",
    "            count += 1\n",
    "            pred = model(x)\n",
    "            val_acc = get_accuracy(pred, y)\n",
    "            total_val_acc += val_acc\n",
    "            \n",
    "        avg_val_acc = total_val_acc / count\n",
    "        \n",
    "    print (\"Epoch: {}, Training Loss: {}, Avg. Validation Accuracy: {}\".format(epoch, epoch_loss, avg_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0adce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(ckpt_dir, cp_name, model):\n",
    "    \"\"\"\n",
    "    Create directory /Checkpoint under exp_data_path and save encoder as cp_name\n",
    "    \"\"\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    saving_model_path = os.path.join(ckpt_dir, cp_name)\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module # convert to non-parallel form\n",
    "    torch.save(model.state_dict(), saving_model_path)\n",
    "    print(f'Model saved: {saving_model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb53921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: v5_trained_real_imgs_classification/v5_model.pt\n"
     ]
    }
   ],
   "source": [
    "exp_dir = f'v5_trained_real_imgs_classification/'\n",
    "save_model(exp_dir, f'v5_model.pt', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd07fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_model.pt\r\n"
     ]
    }
   ],
   "source": [
    "!cd v5_trained_real_imgs_classification && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c00d2bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset at 'dataset/sketchy/sketchy_doodle.npy'.\n",
      "Loaded dataset at 'dataset/tuberlin/tuberlin.npy'.\n",
      "Loaded dataset at 'dataset/google_images/google_doodles.npy'.\n",
      "Loaded dataset at 'dataset/sketchy/sketchy_doodle.npy'.\n",
      "Loaded dataset at 'dataset/tuberlin/tuberlin.npy'.\n",
      "Loaded dataset at 'dataset/google_images/google_doodles.npy'.\n"
     ]
    }
   ],
   "source": [
    "doodle_train_set = DoodleDataset(train=True)\n",
    "doodle_val_set = DoodleDataset(train=False)\n",
    "\n",
    "doodle_train_loader = torch.utils.data.DataLoader(doodle_train_set, batch_size=128, shuffle=True)\n",
    "doodle_val_loader = torch.utils.data.DataLoader(doodle_val_set, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "381e597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = ConvNeXt(1, 9)\n",
    "optim = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb15b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(0)  # zero seed by default\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5cc9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 3.5000380788530623, Avg. Validation Accuracy: 0.12388392857142858\n",
      "Epoch: 1, Training Loss: 2.2491656371525357, Avg. Validation Accuracy: 0.12834821428571427\n",
      "Epoch: 2, Training Loss: 2.2187007665634155, Avg. Validation Accuracy: 0.1640625\n",
      "Epoch: 3, Training Loss: 2.162825516292027, Avg. Validation Accuracy: 0.16350446428571427\n",
      "Epoch: 4, Training Loss: 2.1408002887453352, Avg. Validation Accuracy: 0.20051316273932254\n",
      "Epoch: 5, Training Loss: 2.1166522332600186, Avg. Validation Accuracy: 0.15513392857142858\n",
      "Epoch: 6, Training Loss: 2.0968960353306363, Avg. Validation Accuracy: 0.22544642857142858\n",
      "Epoch: 7, Training Loss: 2.0451242923736572, Avg. Validation Accuracy: 0.19363839285714285\n"
     ]
    }
   ],
   "source": [
    "epochs= 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    count = 0\n",
    "    for idx, (x, y) in enumerate(doodle_train_loader):\n",
    "        count += 1\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        total_loss += loss.detach().cpu().item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    epoch_loss = total_loss / count\n",
    "    \n",
    "    total_val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for idx, (x, y) in enumerate(doodle_val_loader):\n",
    "            count += 1\n",
    "            pred = model(x)\n",
    "            val_acc = get_accuracy(pred, y)\n",
    "            total_val_acc += val_acc\n",
    "            \n",
    "        avg_val_acc = total_val_acc / count\n",
    "        \n",
    "    print (\"Epoch: {}, Training Loss: {}, Avg. Validation Accuracy: {}\".format(epoch, epoch_loss, avg_val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
