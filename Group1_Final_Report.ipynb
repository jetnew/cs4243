{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedbf04a",
   "metadata": {},
   "source": [
    "# Tip-of-the-Tongue: Doodle-Image Retrieval Engine\n",
    "\n",
    "__Group 1:__ \n",
    "- Ai Bo (TODO)\n",
    "- New Jun Jie (TODO)\n",
    "- Rishabh Anand (A0220603Y)\n",
    "\n",
    "---\n",
    "\n",
    "__Tip of the tongue__ refers to the situation when we have a vague idea of an object in our memory but simply cannot name it. Most often than not, we feel retrieval of the object's name is imminent. However, we can definitely draw out a doodle of this object when asked to. The objective of this CS4243 project is to investigate the design of learning algorithms for retrieving a collection of real-world images from these manually drawn doodles. \n",
    "\n",
    "As this module is on Computer Vision, our project focuses on the dataset collection and preprocessing, as well as model selection, training, and testing _only_. One can easily package the models into a search engine that takes in a doodle and returns the top-k matching images.\n",
    "\n",
    "As a taster, here are some interesting results:\n",
    "<!-- ADD RESULTS -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f786eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93e78f",
   "metadata": {},
   "source": [
    "## Training and Testing Dataset\n",
    "\n",
    "The dataset consists of an amalgam of over 1 million doodles web-scraped from the following sources:\n",
    "\n",
    "- Google Quick, Draw!\n",
    "- Sketchy\n",
    "\n",
    "It also features real-life images web-scraped from:\n",
    "\n",
    "- Google Images \n",
    "-\n",
    "\n",
    "Our final dataset is an unpaired "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8781c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_dataset(datasets, size):\n",
    "    combined_dataset = {}\n",
    "    for name, dataset in datasets.items():\n",
    "        for class_name, class_data in dataset.items():\n",
    "            if class_name not in combined_dataset:\n",
    "                combined_dataset[class_name] = []\n",
    "            # resize data so they can be stacked\n",
    "            resized = []\n",
    "            for data in class_data:\n",
    "                resized.append(cv2.resize(data, (size, size), interpolation=cv2.INTER_AREA))\n",
    "            resized = np.stack(resized, axis=0)\n",
    "            combined_dataset[class_name].append(resized)\n",
    "    for class_name, lst_datasets in combined_dataset.items():\n",
    "        combined_dataset[class_name] = np.concatenate(lst_datasets, axis=0)\n",
    "    return combined_dataset\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    DATASET_DIR = {True: 'dataset/dataset_train.npy', False: 'dataset/dataset_test.npy'}\n",
    "\n",
    "    def __init__(self, doodles_list, real_list, doodle_size, real_size, train: bool):\n",
    "        super(ImageDataset, self).__init__()\n",
    "\n",
    "        dataset = np.load(self.DATASET_DIR[train], allow_pickle=True)[()]\n",
    "\n",
    "        doodle_datasets = {name: data for name, data in dataset.items() if name in doodles_list}\n",
    "        real_datasets = {name: data for name, data in dataset.items() if name in real_list}\n",
    "        self.doodle_dict = combined_dataset(doodle_datasets, doodle_size)\n",
    "        self.real_dict = combined_dataset(real_datasets, real_size)\n",
    "\n",
    "        # sanity check\n",
    "        assert set(self.doodle_dict.keys()) == set(self.real_dict.keys()), \\\n",
    "            f'doodle and real images label classes do not match'\n",
    "\n",
    "        # process classes\n",
    "        label_idx = {}\n",
    "        for key in self.doodle_dict.keys():\n",
    "            if key not in label_idx:\n",
    "                label_idx[key] = len(label_idx)\n",
    "        self.label_idx = label_idx\n",
    "\n",
    "        # parse data and labels\n",
    "        self.doodle_data, self.doodle_label = self._return_x_y_pairs(self.doodle_dict, label_idx)\n",
    "        self.real_data, self.real_label = self._return_x_y_pairs(self.real_dict, label_idx)\n",
    "\n",
    "        # data preprocessing\n",
    "        self.doodle_preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(doodle_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((self.doodle_data/255).mean(), (self.doodle_data/255).std())   # IMPORTANT / 255\n",
    "        ])\n",
    "\n",
    "        self.real_preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(real_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((self.real_data/255).mean(axis=(0, 1, 2)), (self.real_data/255).std(axis=(0, 1, 2)))\n",
    "        ])\n",
    "\n",
    "        print(f'Train = {train}. Doodle list: {doodles_list}, \\n real list: {real_list}. \\n classes: {label_idx.keys()} \\n'\n",
    "              f'Doodle data size {len(self.doodle_data)}, real data size {len(self.real_data)}, '\n",
    "              f'ratio {len(self.doodle_data)/len(self.real_data)}')\n",
    "\n",
    "    def _return_x_y_pairs(self, data_dict, category_mapping):\n",
    "        xs, ys = [], []\n",
    "        for key in data_dict.keys():\n",
    "            data = data_dict[key]\n",
    "            labels = [category_mapping[key]] * len(data)\n",
    "            xs.append(data)\n",
    "            ys.extend(labels)\n",
    "        return np.concatenate(xs, axis=0), np.array(ys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # naive sampling scheme - sample with replacement\n",
    "        # sample label first so that doodle and real data belong to the same category\n",
    "        label = random.choice(list(self.label_idx.keys()))\n",
    "        doodle_data = self.doodle_preprocess(random.choice(self.doodle_dict[label]))\n",
    "        real_data = self.real_preprocess(random.choice(self.real_dict[label]))\n",
    "        numer_label = self.label_idx[label]\n",
    "        return doodle_data, numer_label, real_data, numer_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.doodle_data), len(self.real_data))     # could be arbitrary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f68d1228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train = True. Doodle list: ['sketchy_doodle', 'tuberlin', 'google_doodles'], \n",
      " real list: ['sketchy_real', 'google_real', 'cifar']. \n",
      " classes: dict_keys(['airplane', 'car', 'cat', 'dog', 'frog', 'horse', 'truck', 'bird', 'ship']) \n",
      "Doodle data size 7022, real data size 46364, ratio 0.15145371408851696\n",
      "Train = False. Doodle list: ['sketchy_doodle', 'tuberlin', 'google_doodles'], \n",
      " real list: ['sketchy_real', 'google_real', 'cifar']. \n",
      " classes: dict_keys(['airplane', 'car', 'cat', 'dog', 'frog', 'horse', 'truck', 'bird', 'ship']) \n",
      "Doodle data size 1764, real data size 9341, ratio 0.18884487742211756\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-6f0cdeaa2de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoodles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoodle_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-c6741beddd0c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# naive sampling scheme - sample with replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# sample label first so that doodle and real data belong to the same category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mdoodle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoodle_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoodle_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "doodles = ['sketchy_doodle', 'tuberlin', 'google_doodles']\n",
    "reals = ['sketchy_real', 'google_real', 'cifar']\n",
    "\n",
    "doodle_size = 64\n",
    "real_size = 64\n",
    "\n",
    "train_set = ImageDataset(doodles, reals, doodle_size, real_size, train=True)\n",
    "val_set = ImageDataset(doodles, reals, doodle_size, real_size, train=False)\n",
    "\n",
    "print(len(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222a0e8",
   "metadata": {},
   "source": [
    "## Models and Approaches\n",
    "\n",
    "1. Version 1: Multilayer Perceptron Classification\n",
    "2. Version 2: Convolutional Neural Network Classification\n",
    "3. Version 3: Convolutional Neural Network with Contrastive Loss\n",
    "4. Version 4: Convolutional Neural Network with multiple Contrastive Losses\n",
    "5. Version 5: ConvNeXt<sup>1</sup> with multiple Contrastive Losses\n",
    "\n",
    "---\n",
    "\n",
    "<sup>1</sup> Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A ConvNet for the 2020s. arXiv preprint arXiv:2201.03545."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f1430",
   "metadata": {},
   "source": [
    "## Version 1: Multilayer Perceptron Classification\n",
    "\n",
    "The final architecture and pipeline look like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fea5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.2):\n",
    "        super(ExampleMLP, self).__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.l2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.l3 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.l4 = nn.Linear(hid_dim, out_dim)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        x = x.flatten(1) # img to vector\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        feat = x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l4(x)\n",
    "\n",
    "        if return_feats:\n",
    "            return x, feat\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9418365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e679f8",
   "metadata": {},
   "source": [
    "## Version 2: Convolutional Neural Network Classification\n",
    "\n",
    "The final pipeline and architecture look like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "963c64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel, stride, padding=0, bias=True):\n",
    "        super().__init__()        \n",
    "        self.block = nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, \n",
    "                            outchannels, \n",
    "                            kernel_size=kernel, \n",
    "                            stride=stride, \n",
    "                            padding=padding, \n",
    "                            bias=bias\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(outchannels),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a34acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    CHANNELS = [64, 128, 192, 256, 512]\n",
    "    POOL = (1, 1)\n",
    "\n",
    "    def __init__(self, in_c, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layer1 = ConvBlock(in_c, self.CHANNELS[0], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer2 = ConvBlock(self.CHANNELS[0], self.CHANNELS[1], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer3 = ConvBlock(self.CHANNELS[1], self.CHANNELS[2], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer4 = ConvBlock(self.CHANNELS[2], self.CHANNELS[3], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer5 = ConvBlock(self.CHANNELS[3], self.CHANNELS[4], kernel=3, stride=2, padding=1, bias=True)\n",
    "        pool = nn.AdaptiveAvgPool2d(self.POOL)\n",
    "        self.layers = nn.Sequential(layer1, layer2, layer3, layer4, layer5, pool)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.nn = nn.Sequential(\n",
    "                    nn.Linear(self.POOL[0] * self.POOL[1] * self.CHANNELS[4], 64),\n",
    "                    nn.Linear(64, num_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.layers(x)\n",
    "        feats = feats.flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "\n",
    "        if return_feats:\n",
    "            return x, feats\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b8e44e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "ConvNet                                  --\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─ConvBlock: 2-1                    --\n",
      "│    │    └─Sequential: 3-1              1,920\n",
      "│    └─ConvBlock: 2-2                    --\n",
      "│    │    └─Sequential: 3-2              74,112\n",
      "│    └─ConvBlock: 2-3                    --\n",
      "│    │    └─Sequential: 3-3              221,760\n",
      "│    └─ConvBlock: 2-4                    --\n",
      "│    │    └─Sequential: 3-4              443,136\n",
      "│    └─ConvBlock: 2-5                    --\n",
      "│    │    └─Sequential: 3-5              1,181,184\n",
      "│    └─AdaptiveAvgPool2d: 2-6            --\n",
      "├─Dropout: 1-2                           --\n",
      "├─Sequential: 1-3                        --\n",
      "│    └─Linear: 2-7                       32,832\n",
      "│    └─Linear: 2-8                       585\n",
      "=================================================================\n",
      "Total params: 1,955,529\n",
      "Trainable params: 1,955,529\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(100, 3, 64, 64)\n",
    "net = ConvNet(3, 9)\n",
    "y = net(x)\n",
    "print (summary(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37f6d9",
   "metadata": {},
   "source": [
    "## Version 3: Convolutional Neural Network with Contrastive Loss\n",
    "\n",
    "#### Contrastive Loss\n",
    "We follow the Contrastive Loss from SimCLR<sup>2</sup>:\n",
    "\n",
    "$$\n",
    "l_{i, j} = -\\log \\frac{\\text{exp}(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{2N}^{k=1} \\mathbb{1}_{k \\neq i} \\text{ exp}(\\text{sim}(z_i, z_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "The total loss is the arithmetic mean of the losses for all positive pairs in a batch:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2N} \\sum^{N}_{k=1} [l(2k-1, 2k) + l(2k, 2k-1)]\n",
    "$$\n",
    "\n",
    "The architecture and pipeline look like so:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### References\n",
    "<sup>2</sup> Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6115d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel, stride, padding=0, bias=True):\n",
    "        super().__init__()        \n",
    "        self.block = nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, \n",
    "                            outchannels, \n",
    "                            kernel_size=kernel, \n",
    "                            stride=stride, \n",
    "                            padding=padding, \n",
    "                            bias=bias\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(outchannels),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    CHANNELS = [64, 128, 192, 256, 512]\n",
    "    POOL = (1, 1)\n",
    "\n",
    "    def __init__(self, in_c, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layer1 = ConvBlock(in_c, self.CHANNELS[0], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer2 = ConvBlock(self.CHANNELS[0], self.CHANNELS[1], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer3 = ConvBlock(self.CHANNELS[1], self.CHANNELS[2], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer4 = ConvBlock(self.CHANNELS[2], self.CHANNELS[3], kernel=3, stride=2, padding=1, bias=True)\n",
    "        layer5 = ConvBlock(self.CHANNELS[3], self.CHANNELS[4], kernel=3, stride=2, padding=1, bias=True)\n",
    "        pool = nn.AdaptiveAvgPool2d(self.POOL)\n",
    "        self.layers = nn.Sequential(layer1, layer2, layer3, layer4, layer5, pool)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.nn = nn.Sequential(\n",
    "                    nn.Linear(self.POOL[0] * self.POOL[1] * self.CHANNELS[4], 64),\n",
    "                    nn.Linear(64, num_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.layers(x)\n",
    "        feats = feats.flatten(1)\n",
    "        x = self.nn(self.dropout(feats))\n",
    "\n",
    "        if return_feats:\n",
    "            return x, feats\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b594399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim_matrix(feats):\n",
    "    \"\"\"\n",
    "    Takes in a batch of features of size (bs, feat_len).\n",
    "    \"\"\"\n",
    "    sim_matrix = F.cosine_similarity(feats.unsqueeze(2).expand(-1, -1, feats.size(0)),\n",
    "                                     feats.unsqueeze(2).expand(-1, -1, feats.size(0)).transpose(0, 2),\n",
    "                                     dim=1)\n",
    "\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def compute_target_matrix(labels):\n",
    "    \"\"\"\n",
    "    Takes in a label vector of size (bs)\n",
    "    \"\"\"\n",
    "    label_matrix = labels.unsqueeze(-1).expand((labels.shape[0], labels.shape[0]))\n",
    "    trans_label_matrix = torch.transpose(label_matrix, 0, 1)\n",
    "    target_matrix = (label_matrix == trans_label_matrix).type(torch.float)\n",
    "\n",
    "    return target_matrix\n",
    "\n",
    "\n",
    "def contrastive_loss(pred_sim_matrix, target_matrix, temperature):\n",
    "    return F.kl_div(F.softmax(pred_sim_matrix / temperature).log(), F.softmax(target_matrix / temperature),\n",
    "                    reduction=\"batchmean\", log_target=False)\n",
    "\n",
    "\n",
    "def compute_contrastive_loss_from_feats(feats, labels, temperature):\n",
    "    sim_matrix = compute_sim_matrix(feats)\n",
    "    target_matrix = compute_target_matrix(labels)\n",
    "    loss = contrastive_loss(sim_matrix, target_matrix, temperature)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88965d",
   "metadata": {},
   "source": [
    "## Version 4: Convolutional Neural Network with multiple Contrastive Losses\n",
    "\n",
    "We add two more losses to the Contrastive Loss from Version 3.\n",
    "\n",
    "#### Loss 2\n",
    "\n",
    "#### Loss 3\n",
    "\n",
    "The final architecture and pipeline look like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a108ec",
   "metadata": {},
   "source": [
    "## (BONUS) Version 5: ConvNeXt with multiple Contrastive Losses\n",
    "\n",
    "<!-- TODO: Talk about ConvNeXt -->\n",
    "\n",
    "Finally, we train ConvNeXt with the three losses used in Version 4. ConvNeXt (CVPR, 2022) is a \"modernised\" ConvNet that hopes of competing head-on with Transformers and their success. We believe using ConvNeXt is a timely choice given its recent success at the CVPR 2022 conference and how simple\n",
    "\n",
    "ConvNeXt is an improvement over the standard ConvNet that brings together innovations from the Transformer<sup>3</sup> and ResNet<sup>4</sup> – the primary work horses in Computer Vision today. Here are the list of enhancements (and their inspiration source) we wish to showcase in this CS4243 project:\n",
    "\n",
    "1. Block-based architecture design (Transformer & Resnet)\n",
    "2. Residual Connections between start and end of block (Transformer & ResNet)\n",
    "3. Wider receptive fields from (3,3) to (7,7) (ResNet)\n",
    "4. Use of GELU activation instead of ReLU (Transformer)\n",
    "5. Substituting BatchNorm with LayerNorm (Transformer)\n",
    "\n",
    "The final architecture and pipeline look like so:\n",
    "\n",
    "\n",
    "\n",
    "*__Note:__ Before ConvNeXt, we considered the Vision Transformer<sup>5</sup> (ViT; ICLR, 2021) for Version 5 but decided against the move because the leap from vanilla CNN to ViT would have been too huge to explain and justify; furthermore, there is no basis of comparison between them, causing our improvements to be null and void. It would be akin to comparing the efficacy of cutting a cake with a kitchen knife and a chainsaw. ConvNeXt provided the perfect middle ground!*\n",
    "\n",
    "---\n",
    "\n",
    "#### References\n",
    "<sup>3</sup> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
    "\n",
    "<sup>4</sup> He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n",
    "\n",
    "<sup>5</sup> Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. International Conference on Learning Representations, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e6b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(dim, dim, (7,7), padding=3, groups=dim)\n",
    "        self.lin1 = nn.Linear(dim, 4 * dim)\n",
    "        self.lin2 = nn.Linear(4 * dim, dim)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_inp = x\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 2, 3, 1) # NCHW -> NHWC\n",
    "        x = self.ln(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = x.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "        out = x + res_inp\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa293f9",
   "metadata": {},
   "source": [
    "We believe __2__ ConvNeXt blocks are comparable in terms of model size (number of parameters) and depth to the vanilla ConvNet used in Version 3 and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0aecb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXt(nn.Module):\n",
    "    def __init__(self, in_channels, classes, block_dims=[192, 384]):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, block_dims[0], kernel_size=2, stride=2),\n",
    "                        ConvNeXtBlock(block_dims[0]),\n",
    "                        nn.Conv2d(block_dims[0], block_dims[1], kernel_size=2, stride=2),\n",
    "                        ConvNeXtBlock(block_dims[1]),\n",
    "                    )\n",
    "        self.block_dims = block_dims\n",
    "        self.project = nn.Linear(block_dims[-1], classes)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        feats = self.blocks(x)\n",
    "        x = feats.view(-1, self.block_dims[-1], 16*16).mean(2)\n",
    "        out = self.project(x)\n",
    "\n",
    "        return out, feats if return_feats else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a746e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "ConvNeXt                                 --\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Conv2d: 2-1                       2,496\n",
      "│    └─ConvNeXtBlock: 2-2                --\n",
      "│    │    └─Conv2d: 3-1                  9,600\n",
      "│    │    └─Linear: 3-2                  148,224\n",
      "│    │    └─Linear: 3-3                  147,648\n",
      "│    │    └─LayerNorm: 3-4               384\n",
      "│    │    └─GELU: 3-5                    --\n",
      "│    └─Conv2d: 2-3                       295,296\n",
      "│    └─ConvNeXtBlock: 2-4                --\n",
      "│    │    └─Conv2d: 3-6                  19,200\n",
      "│    │    └─Linear: 3-7                  591,360\n",
      "│    │    └─Linear: 3-8                  590,208\n",
      "│    │    └─LayerNorm: 3-9               768\n",
      "│    │    └─GELU: 3-10                   --\n",
      "├─Linear: 1-2                            3,465\n",
      "=================================================================\n",
      "Total params: 1,808,649\n",
      "Trainable params: 1,808,649\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(100, 3, 64, 64)\n",
    "net = ConvNeXt(3, 9)\n",
    "y, _ = net(x)\n",
    "print (summary(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb2cba",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "While the quality of real-life images returned by the model for a given doodle is subjective, we use classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4b064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3171dcb2",
   "metadata": {},
   "source": [
    "## Results and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2672f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1477dbdf",
   "metadata": {},
   "source": [
    "## Analysis and Ablations\n",
    "\n",
    "### t-SNE\n",
    "We wish to analyse how the MLP generates embeddings for doodles and real images respectively. We use the $t$-Stochastic Neighbour Embeddings (t-SNE) method to visualise the output embeddings on the validation set in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_set, batch_size=256, shuffle=False, drop_last=True)\n",
    "\n",
    "doodle_model = DoodleMLP(doodle_size * doodle_size, 128, NUM_CLASSES, dropout=0.2)\n",
    "real_model = RealMLP(real_size * real_size * 3, 512, NUM_CLASSES, dropout=0.2)\n",
    "model1 = load_model_dic(doodle_model, \"mlp_trained/14_model1.pt\")\n",
    "model2 = load_model_dic(real_model, \"mlp_trained/14_model2.pt\")\n",
    "\n",
    "model1.eval(), model1.eval()\n",
    "running_loss1 = 0.0\n",
    "running_loss2 = 0.0\n",
    "\n",
    "for i, (x1, y1, x2, y2) in enumerate(val_loader):\n",
    "    pred1, feats1 = model1(x1, return_feats=True)\n",
    "    pred2, feats2 = model2(x2, return_feats=True)\n",
    "    running_loss1 += compute_accuracy(pred1, y1)\n",
    "    running_loss2 += compute_accuracy(pred2, y2)\n",
    "avg_loss1 = running_loss1 / (i + 1)\n",
    "avg_loss2 = running_loss2 / (i + 1)\n",
    "print(f\"Doodle acc: {avg_loss1:.4f}, Real acc: {avg_loss2:.4f}\")\n",
    "\n",
    "xs1 = []\n",
    "xs2 = []\n",
    "fs1 = []\n",
    "fs2 = []\n",
    "ys1 = []\n",
    "ys2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x1, y1, x2, y2) in enumerate(val_loader):\n",
    "        pred1, feats1 = model1(x1, return_feats=True)\n",
    "        pred2, feats2 = model2(x2, return_feats=True)\n",
    "        xs1.append(x1)\n",
    "        xs2.append(x2)\n",
    "        fs1.append(feats1)\n",
    "        fs2.append(feats2)\n",
    "        ys1.append(y1)\n",
    "        ys2.append(y2)\n",
    "data1 = torch.cat(xs1).numpy()\n",
    "data2 = torch.cat(xs2).numpy()\n",
    "feats1 = torch.cat(fs1).numpy()\n",
    "labels1 = torch.cat(ys1).numpy()\n",
    "feats2 = torch.cat(fs2).numpy()\n",
    "labels2 = torch.cat(ys2).numpy()\n",
    "\n",
    "def plot_tsne(feats, labels, pc=0, size=1, alpha=1):\n",
    "    if pc > 0:\n",
    "        feats = PCA(n_components=pc).fit_transform(feats)\n",
    "    c = TSNE(n_components=2).fit_transform(feats)\n",
    "    CLASSES = [\"airplane\", \"car\", \"bird\", \"cat\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    class_to_idx = {k: i for i, k in enumerate(CLASSES)}\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(CLASSES)))\n",
    "    plt.title(\"TSNE of MLP embeddings\")\n",
    "    for clas, color in zip(CLASSES, colors):\n",
    "        idx = np.where(labels == class_to_idx[clas])[0]\n",
    "        plt.scatter(c[idx][:,0], c[idx][:,1], label=clas, s=size, alpha=alpha, color=color)\n",
    "    plt.legend(loc='best', markerscale=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fee4d3",
   "metadata": {},
   "source": [
    "### GradCAM on ConvNet and ConvNeXt\n",
    "\n",
    "This project's major contributions lie in the CNNs trained above. As with MLPs and t-SNE, we complement all our convolutional models (Version 2, 3, 4, 5) with GradCAM, a method to visualise where the convolutional model is focusing on. It looks out for the most salient features in the image and predicts a class majorly influenced by said features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CAM(feature_map, weight, class_idx):\n",
    "    size_upsample = (32, 32)\n",
    "    bz, nc, h, w = feature_map.shape\n",
    "\n",
    "    before_dot = feature_map.reshape((nc, h*w))\n",
    "    cam = weight[class_idx].unsqueeze(0) @ before_dot\n",
    "\n",
    "    cam = cam.squeeze(0)\n",
    "    cam = cam.reshape(h, w)\n",
    "    cam = cam - torch.min(cam)\n",
    "    cam = cam / torch.max(cam)\n",
    "    cam = torch.clip(cam, 0, 1)\n",
    "    \n",
    "    img = transforms.Resize(size_upsample)(cam.unsqueeze(0))\n",
    "    \n",
    "    return img.detach().numpy(), cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "def plot_cam(img, cam):\n",
    "    ''' Visualization function '''\n",
    "    img = img.permute(1, 2, 0)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10,7))\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title(f\"Input image\\nLabel: {cifar10_classes[y]}\")\n",
    "\n",
    "    ax2.imshow(cam.reshape(32, 32), cmap=\"jet\")\n",
    "    ax2.set_title(\"Raw CAM.\")\n",
    "\n",
    "    ax3.imshow(img)\n",
    "    ax3.imshow(cam.reshape(32, 32), cmap=\"jet\", alpha=0.2)\n",
    "    ax3.set_title(f\"Overlayed CAM.\\nPrediction: {cifar10_classes[idx[0]]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5772623",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = torch.randint(0, 10000, size=[1]) # pick a random index from the test set\n",
    "\n",
    "x = val_[rand_idx][0] # test image\n",
    "y = cifar_test[rand_idx][1] # associated test label\n",
    "\n",
    "model.eval()\n",
    "scores = model(x.unsqueeze(0)) # get the soft labels\n",
    "probs = scores.data.squeeze()\n",
    "probs, idx = probs.sort(0, True)\n",
    "\n",
    "print('true class: ', cifar10_classes[y])\n",
    "print('predicated class: ', cifar10_classes[idx[0]])\n",
    "\n",
    "params = list(model.fc.parameters())\n",
    "weight = params[0].data\n",
    "\n",
    "feature_maps = model.conv(x.unsqueeze(0))\n",
    "heatmap, _ = get_CAM(feature_maps, weight, idx[0])\n",
    "    \n",
    "plot_cam(x, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf85f9",
   "metadata": {},
   "source": [
    "## Test-driving the Similarity Engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
